---
title:  GAN Loss
categories:
- paper-reading
tags:
- GAN
- loss
---

&emsp;&emsp;看完了sinGAN的源码之后, 针对其GAN loss的形式有些不解, 所以找了一些关于GAN loss的文献资料, 在此做一些记录.
<!-- more -->

***
### GAN在训练中存在的问题
>+ *梯度消失问题*: 在训练的起步阶段, G生成的$X_{fake}$与$X_{real}$差距太大, 一眼就被D看出来是假的, 即无论G生成什么, 都被D判定为概率0, 由此产生的梯度是常数, 无法对G的更新起到帮助. 与此同时, D的判别本领也越来越强, 导致一个恶性循环, 训练无法进行下去, 即梯度消失现象. 形象地来讲, 就是G不知道生成什么样的图像才可能使D认错, D根本就不给错误发生的机会, 也就不知道该往哪个方向去优化. 这种现象发生的原因是G生成的$X_{fake}$与$X_{real}$数据分布相差过大, 没有重叠区域, 也就无法使D认错. (真图很真 & 假图很假)
>+ *mode collapse现象*: 某些情况下, G偶然生成了一张图片, 突然发现D会错认这张图片, 就好像捡到宝了, 然后就变得懒惰, 倾向于生成该张图片或与之极其类似的图片, 也能使训练进行下去, 最终导致生成的图像缺乏多样性, 重复性高, 即mode collapse. (有点类似于陷入了局部次优解)

### 从公式出发, 找出问题
>+ 在上一篇针对[sinGAN的介绍][4]中, 对GAN loss有了初步的分析. 其中, 针对生成器的loss如下:
$log(1-D(X_{fake}))$ (公式1)  
&emsp;&emsp;其等价形式可为:  
$-logD(X_{fake})$ (公式2)  
&emsp;&emsp;我们基于这两个loss出发, 探究一下问题存在的原因.  
<br
/>
&emsp;&emsp;针对公式1,  
&emsp;&emsp;其加上一个与生成器无关的量, 变成等价形式(易于后面推导)  
$log(D(X_{real})) + log(1-D(X_{fake}))$ (公式3)  
&emsp;&emsp;对交叉熵$-P_r(X)logD(X)-P_g(X)log(1-D(X))$关于$D(X)$求导, 得:  
$-\frac{P_r(X)}{D(X)} + \frac{P_g(X)}{1-D(X)} = 0$  
&emsp;&emsp;化简出最优D的形式为:  
$D^{\ast}(X) = \frac{P_r(X)}{P_r(X)+P_g(X)}$  
&emsp;&emsp;将上式带入公式3中, 得,    
$log \frac{P_r(X)}{\frac12(P_r(X)+P_g(X))} - log2 + log \frac{P_g(X)}{\frac12(P_r(X)+P_g(X))} - log2$  (公式3)  
&emsp;&emsp;其中, 定义两个量  
KL散度:  $KL(P_1 \parallel P_2) = E(log\frac{P_1}{P_2})$  
JS散度:  $\frac12KL(P_1 \parallel \frac{P_1+P_2}{2}) + \frac12KL(P_2 \parallel \frac{P_1+P_2}{2})$  
&emsp;&emsp;因此公式3可以化简为,  
$2JS(P_r \parallel P_g) - 2log2$  
&emsp;&emsp;这时候问题可以看出来了, 这里为了方便引用知乎上的一篇[专栏][1]图如下,  
![](/assets/images/GANloss/1.png)
&emsp;&emsp;这就是梯度消失的问题.  
<br
/>
&emsp;&emsp;针对公式2,  
&emsp;&emsp;先利用以下公式作为桥梁,  
$KL(P_g \parallel P_r) = E(log\frac{P_g}{P_r}) = E(log\frac{1-D^\ast(X)}{D^\ast(X)}) = E(log(1-D^\ast(X))) - E(logD^\ast(X))$  
&emsp;&emsp;所以,  
$E(-logD^\ast(X)) = KL(P_g \parallel P_r) - E(log(1-D^\ast(X))) = KL(P_g \parallel P_r) - [2JS(P_r \parallel P_g) - 2log2 - log(D^\ast(X_{real}))]$  
&emsp;&emsp;去掉上式中与生成器无关的量, 得到化简式,  
$KL(P_g \parallel P_r) - 2JS(P_r \parallel P_g)$  
&emsp;&emsp;这里, 再引用一下知乎[专栏][1]里的解释:  
![](/assets/images/GANloss/2.png)
&emsp;&emsp;这就是mode collapse现象.

### 解决方法: Wasserstein GAN Loss
>+ 思路一: 既然由于$X_{fake}$和$X_{real}$存在重叠的部分少会产生问题, 那么就人为强制地让它们有交集. 例如, 加噪声, 引用知乎[专栏][1]里的解释: `对生成样本和真实样本加噪声，直观上说，使得原本的两个低维流形“弥散”到整个高维空间，强行让它们产生不可忽略的重叠`.  
>+ 思路二: [Wasserstein GAN][2]一文抛弃了KL散度和JS散度, 重新定义了一个新的距离`Earth-Mover距离`(推土机距离), 这个距离即使在两个分布之间没有重叠部分时, 它也有值能反映距离的远近.
![](/assets/images/GANloss/3.png)
&emsp;&emsp;在这个距离作为衡量的准则下, 经过一系列的理论推导, loss函数有如下形式:  
&emsp;&emsp;更新D时:  $loss=D(X_{fake}) - D(X_{real})$  
&emsp;&emsp;更新G时:  $loss=-D(X_{fake})$  
&emsp;&emsp;与普通GAN loss相比, 只改了四点:
![](/assets/images/GANloss/4.png)
&emsp;&emsp;该方案基本解决了普通GAN loss所遇到的梯度消失和mode collapse问题.
>+ 思路三: 在思路二中, 对参数绝对值进行clip, 随后发现这样会导致网络参数倾向于在clip范围内极端分布, 成为一个二值神经网络, 显著降低了网络容量. 而且一旦clip范围控制得不够好, 还是容易导致梯度爆炸或消失. 随即, 该文作者后发表了[Improved Wasserstein GAN][3]作为改进, 将"强行clip"改为"gradient penalty", 并称这种新loss为`WGAN-GP`, 即在[sinGAN的介绍][4]一文中提及到的loss.  
&emsp;&emsp;penalty项的具体形式:   
$[\parallel \bigtriangledown D(X) \parallel - K]^2$  
&emsp;&emsp;具体到代码中, 
![](/assets/images/GANloss/5.png)
&emsp;&emsp;由此, 基本理解了[sinGAN的介绍][4]中关于loss中不理解的地方.

<br
/>
##### 参考资料:
GAN综述: [How Generative Adversarial Networks and Their Variants Work: An Overview](https://arxiv.xilesou.top/pdf/1711.05914.pdf)    
机器之心关于GAN综述的整理: [万字综述之生成对抗网络（GAN）](https://www.jiqizhixin.com/articles/2019-03-19-12?from=synced&keyword=GAN)  
知乎专栏: [令人拍案叫绝的Wasserstein GAN][1]  
知乎问题: [Improved Wasserstein GAN](https://www.zhihu.com/question/52602529/answer/158727900)  
Wasserstein GAN论文: <https://arxiv.org/pdf/1701.07875.pdf>  
Improved Wasserstein GAN论文: <https://arxiv.org/pdf/1704.00028.pdf>  

[1]: https://zhuanlan.zhihu.com/p/25071913
[2]: https://arxiv.org/pdf/1701.07875.pdf
[3]: https://arxiv.org/pdf/1704.00028.pdf
[4]: https://fengweiustc.github.io/paper-reading/2019/11/03/sinGan/#



