---
title:  YOLO v1/v2/v3/v4
categories:
- paper-reading
tags:
- detection
- loss
---

&emsp;&emsp;YOLO v1-v3的作者，Joseph Redmon大神，因为考虑到自己的研究成果会被用于军事用途和个人隐私领域，因此在2020年2月宣布退出CV界，可谓是非常洒脱了。但YOLO系列并没有因此停止更新，在随后的4月，YOLO v4由他人接棒发表并得到了大神本人的认可。全文涉及到非常多的detection领域的历史研究成果，可谓是集大成者，一开始初看简直是眼花缭乱，也真心感叹现在水论文越来越不容易了。为了理解这篇paper也补看了很多相关的历史paper，中间又断断续续地受到疫情和毕业手续的影响，现在终于能静下心来好好地记录一下相关的知识。全文涵盖的知识点比较多且杂，但也是成体系的，久远的YOLO v1-v3会一带而过，重点放在v4上（李鬼版的v5请忽略）。
<!-- more -->

***
>+ # YOLO v1（CVPR 2016）

&emsp;&emsp;早在关于[ThunderNet](https://fengweiustc.github.io/paper-reading/2019/11/06/detection/)的一文中，就已经对YOLO v1的工作进行了记录。  
&emsp;&emsp;它的优点在于，第一次把detection这样的任务用端到端的形式来完成，能达到很高的计算速度。
![](/assets/images/yolo/1.png)
&emsp;&emsp;它的缺点在于：一是网络中存在全连接层，这样与空间位置相对应的信息会被打乱，localization会受影响，因此它的框位置误差很大；二是划分成7x7的网格后每个网格只预测B=2个候选框，数量这么少会有很快的计算速度，但这导致了很低的召回率，会漏掉很多小目标（如密集的鸟群场景），另外由于缺少特定长宽比的候选框（如Faster-RCNN中的9种指定长宽比的候选框），缺少了这部分的先验知识，对于极端长宽比的物体检测效果会很差（如细长的领带）；三是计算loss所用的参考量（x，y，w，h）没有归一化，此时“大框的小误差”和“小框的大误差”的损失值几乎相同，但是后者对精度的提升明显更重要。  

原文地址：[You Only Look Once: Unified, Real-Time Object Detection](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf)


***
>+ # YOLO v2（CVPR 2017）

&emsp;&emsp;v2在v1的基础上，分别针对训练trick（Better）、网络结构本身（Faster）、数据集（Stronger）三个方向做了改进。

>> ## 训练trick（Better）  

&emsp;&emsp;-- 使用了BN。    
&emsp;&emsp;-- v1的backbone在ImageNet上预训练时，输入尺寸是224x224，而迁移到detection任务后输入尺寸变成了448x448，这之间的转变太过突然。为此，v2在预训练时先在448x448的尺寸上跑了10个epoch作为过渡，这样能涨4个点。这提醒我们，不仅train和inference时要保持前后一致，pre-train和fine tune也需要保持一致。   
&emsp;&emsp;-- 去掉了v1中的全连接层；输入从448x448改为奇数的416x416，以方便取几何中心；取更大的feature（13x13，v1中为7x7）来作为最终的特征层；抛弃v1中的anchor free，重拾anchor box，使得召回率有所提升。   
&emsp;&emsp;-- 为了给予合理的检测框先验信息，使用k-means聚类来对VOC和COCO数据集中的标签框进行分析，得出了k=5时的几种常见框尺寸，使得无需手动挑选候选框。  
&emsp;&emsp;-- 坐标值采用归一化后的有限相对值，提升模型训练的稳定性。v1中的坐标值是绝对值（没有归一化）缺乏对小框的关注，RPN中的坐标值是相对值（但没有对范围做限制）在模型训练初期具有不稳定性，v2则吸取两种方法的优缺点，预测坐标相对于grid cell的相对偏移量，这种偏移还经过归一化处理，不会超过0-1的范围。
![](/assets/images/yolo/2.png)
![](/assets/images/yolo/3.png)  
&emsp;&emsp;-- 融合最后的13x13和倒数第二层的26x26的多尺度feature，兼顾大目标和小目标。  
&emsp;&emsp;-- 每10个batch变换一次输入图片尺寸，从最小的320x320逐渐变到到最大的608x608，提升模型对多尺度目标的检测能力。由于网络中不含全连接层，因此训练时可采用不同大小的尺寸。  

>> ## 网络结构（Faster）

&emsp;&emsp;可能是大佬为了推广自己写的框架[Darknet](https://pjreddie.com/darknet/)，v2的backbone并没有采用VGG，而是自定义了一个很朴素的网络，命名为Darknet-19，flops要比VGG小。整体看来平平无奇。
![](/assets/images/yolo/4.png)  

>> ## 数据集（Stronger）

&emsp;&emsp;数据比模型更重要。ImageNet的建立为分类网络的不断突破提供了平台，但在detection任务上，标数据更为繁琐，一张图片可能有很多个物体，每个物体的种类及边界框都需要标出来，工作量还是很大的，因此现有的detection公开数据集（PASCAL VOC、MSCOCO、ImageNet Det等）规模远不及classification数据集（ImageNet等）。那么有没有办法把classification数据集也利用起来？毕竟它们虽然没有提供坐标信息，但是也提供了类别信息，这部分类别信息能够显著拓展检测的类别数。  
&emsp;&emsp;根据这种思想，v2将detection数据集和classification数据集融合起来。当输入是detection数据时，按照正常的训练过程进行反向传播；当输入是残缺的classification数据时，只计算和更新类别对应的loss和网络参数。
![](/assets/images/yolo/5.png)
&emsp;&emsp;难点在于，如何对数据集进行融合？如上图，用于detection的COCO数据集和用于classification的ImageNet都是属于摊大饼型的结构，每一个类别之间地位平等且相互排斥。但两个数据集之间存在交叉部分，同时由于ImageNet的细粒度分类很全，例如“英短蓝白猫”、“英短金渐层”同属于“英短猫”，“英短猫”又属于“猫”，“猫”又属于“动物”，动物又属于“物体”。因此采用层级的树状关系图来处理融合后的数据比较合理。  
&emsp;&emsp;文中将COCO和ImageNet融合后，按照树状图的关系处理，得到具有1396个节点的树。针对每一个anchor，预测出长度为1396的矢量，对该矢量按照层级关系进行同层级的softmax得到条件概率，根据全概率公式连乘即可得到所属类别的条件概率。
![](/assets/images/yolo/6.png)
![](/assets/images/yolo/7.png)
&emsp;&emsp;经过这种弱监督式的融合数据集训练后，YOLO v2得以检测超过9000种类别，因此得名为YOLO 9000。  
<br/>
&emsp;&emsp;总结一下，v2的主要特点在于使用了融合数据集，一下子拓展了可检测的类别范围。至于网络结构和训练trick，感觉算是小的改进和调参吧。  
&emsp;&emsp;原文地址：[YOLO9000: Better, Faster, Stronger](http://openaccess.thecvf.com/content_cvpr_2017/papers/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.pdf)

***
>+ # YOLO v3（2018）

&emsp;&emsp;v3是以会议报告的形式发表的，所以写作比较随意。主要的改进是吸收了ResNet残差连接、FPN多尺度特征的思想，对网络结构进行了修改，其它更多的是一些有利于工程化的trick。

>> ## 网络结构 

![](/assets/images/yolo/8.png)
&emsp;&emsp;类似于v2中的Darknet-19，但是加入了skip connection后，网络得以变得更深，共有53层，故取名为Darknet-53。
![](/assets/images/yolo/9.jpg)   
&emsp;&emsp;在backbone后加入FPN，在三个尺度上进行检测（这里图方便盗个图，出处在[知乎的一篇博客](https://zhuanlan.zhihu.com/p/76802514)）。

>> ## 训练trick

&emsp;&emsp;-- 类似于v2，对数据集的标签框进行聚类，但是这里取k=9，得到如下的先验框尺寸：(10×13),(16×30),(33×23),(30×61),(62×45),(59×119),(116 × 90),(156 × 198),(373 × 326)。  
&emsp;&emsp;-- 放弃了在v2中使用的多层级softmax，因为作者认为用这种方法求得各个类别的概率其实效果不好，转而采用独立的logistic classifier分类器进行分类。  
&emsp;&emsp;-- 把anchor box分成三类：正例，与任一ground truth之间的IoU最高的视为正例，可用来计算置信度、检测框、类别loss；负例，与所有ground truth之间的IoU都小于阈值（0.5）视为负例，不用于计算loss；忽略例，除去正例后，与任一ground truth之间的IoU大于阈值（0.5）视为忽略例，不用于计算loss。  
&emsp;&emsp;-- 检测框loss采用MSE，类别loss采用BCE，置信度loss采用BCE。
![](/assets/images/yolo/10.png)
<br/>
&emsp;&emsp;总结一下，v1中小目标召回率低的问题，在v3中由于加了带先验的anchor，已经得到了较好的解决；但是在localization的问题上依然有所欠缺，这算是one-stage方法固有的缺陷吧，不过速度依然是快的飞起。  
&emsp;&emsp;原文地址：[YOLOv3: An Incremental Improvement](https://arxiv.org/pdf/1804.02767.pdf)


***
>+ # YOLO v4（2020）

&emsp;&emsp;2020年4月份预发表，使用了很多最新的通用模块，具有技术的后发优势，通过这些模块的排列组合，调参调出了一种最优模型。当然也有作者创新的模块，例如新的数据增广方式Mosaic和Self-Adversarial Training、改进后的SAM/PAN/Cross mini-Batch BN等。性能上比v3的AP高了10%，FPS高了12%。  
&emsp;&emsp;在结构上，把可选用的模块按位置分为了如下图的几个区域；在计算速度上，把模块分成了Bag of freebies（只增加training时间，不增加inference速度，如数据增广、新loss）和Bag of specials（既增加training时间，又增加inference速度，如可增大感受域的模块、attention模块、多尺度特征融合、新的激活函数和后处理方法）。
![](/assets/images/yolo/11.png)
&emsp;&emsp;下面按照Bag of freebies/specials的分类来逐个介绍各个模块，比较熟悉的经典模块会跳过，之前了解较少的模块会单独介绍。

>> ## Bag of freebies

>>> ### 数据增广

&emsp;&emsp;-- 像素级photometric distortions：调整图片的brightness, contrast, hue, saturation, and noise。  
&emsp;&emsp;-- 像素级geometric distortions：对图片进行random scaling, cropping, flipping, and rotating.  
&emsp;&emsp;-- 区域级：random erase、CutOut、hide-and-seek、grid mask、MixUp、CutMix、style transfer GAN（详见[texture-shape cue conflict of CNN](https://fengweiustc.github.io/paper-reading/2020/06/19/texture/)一文）。

>>> ### 数据不平衡

&emsp;&emsp;-- two stage：hard negative example mining、online hard example mining。  
&emsp;&emsp;-- one stage：focal loss。  
&emsp;&emsp;-- 其他：label smoothing。  
&emsp;&emsp;这里针对label-smoothing regularization(LSR)做一下详细记录。  
&emsp;&emsp;在分类问题中，常用交叉熵作为loss。标签值是互不相同的整数，在计算预测的分布$p(k)$时，用softmax使大的预测值越大，小的预测值越小，迫使它们逼近整数的标签值，这是一种硬分类。硬分类带来的后果就是容易过拟合，类似于没有软间隔的支持向量机。
![](/assets/images/yolo/12.png)
&emsp;&emsp;为了减弱过拟合的风险，增强泛化能力，需要把硬分类转化为软分类。原先的真实分布如下，它非0即1：
![](/assets/images/yolo/13.png)
&emsp;&emsp;为了软化它，使它介于0-1之间，在后增加一个先验项$u(k)$，并分配权重$\epsilon$，形式如下：
![](/assets/images/yolo/14.png)
&emsp;&emsp;这个先验项可定义为均匀分布$u(k)=1/K$，当$\delta=0$时，标签分布$q$不至于为0；当$\delta=1$时，$q$也不至于为1，而是介于0-1之间的值。相当于一方面既要使预测分布$p$靠近标签分布$q$，一方面又要使$p$靠近均匀分布$u$，使得类间分布尽量平衡，不偏向任何一类，减少过拟合的倾向。[原文(Inception-v3)](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf)中设$\epsilon=0.1$，在ILSVRC 2012上的误差能降低0.6个点。

>>> ### 损失函数

&emsp;&emsp;有常见的MSE loss，以及各种尺度归一化的IoU loss的变体，如GIoU、DIoU、CIoU（详见[G/D/C-IoU loss](https://fengweiustc.github.io/paper-reading/2020/06/20/iou-loss/#)一文）。

>> ## Bag of specials

>>> ### 增大感受域

&emsp;&emsp;主要有SPP、ASPP、RFB。  
&emsp;&emsp;这里针对 ***RFB（Receptive Field Block，ECCV 2018）*** 做一下记录。  
&emsp;&emsp;首先，作者炒了一个概念，以仿生学为依据，发现人类视网膜细胞组织中偏心率越大的地方（远离视野中心），感光细胞体积越大，分布越稀疏；换而言之，偏心率越小的地方，感光细胞体积越小越密集。这里的视网膜可视作CNN中的感受域，偏心率对应着偏离感受域中心的程度，感光细胞体积对应着卷积核的大小。
![](/assets/images/yolo/15.png)
&emsp;&emsp;存在即合理，更何况是存在于人的视网膜细胞上。为了模仿出上图这种感受域，作者设计出了如下图的RFB模块。首先一上来是跟Inception一样的多级kernel Conv，Conv的kernel越大，对应着越大体积的感光细胞；然后紧接着是dilation rate不同的atrous Conv（注意kernel大小都为3x3），不同的rate对应着感受域上不同的偏心率；最后三个分支的feature叠加在一起，就形成了中间小且密集、外围大且稀疏的感受域分配。不管它有没有道理，模仿就完事了。
![](/assets/images/yolo/16.png)
&emsp;&emsp;作者也比较了Inception、ASPP、Deformable Conv、RFB这四种多尺度感受域融合模块的结构（如下图）。这其中，跟RFB最接近的就是ASPP了，不同的地方在于ASPP第一层的Conv kernel大小都相同，相当于视野中无论远近如何都是体积大小相同的感光细胞。而Inception则缺少atrous Conv，相当于不同大小的感光细胞都堆到一起了，没有被偏心率所分配开。也就是说，RFB对视网膜的模仿最像。
![](/assets/images/yolo/17.png)
&emsp;&emsp;由此，可构造RFB和RFB-Net。其中，RFB-s模仿的是人类浅层视觉细胞的感受域，细胞核的体积更小，用1xn和nx1代替大的nxn kernel，分支也更多。RFB-Net中，RFB-s也是接在浅层feature后，RFB接在深层feature后。最终的模型效果也还不错，侧重于轻量级且高效提取特征。
![](/assets/images/yolo/18.png)
![](/assets/images/yolo/19.png)


原文地址：[Receptive Field Block Net for Accurate and Fast Object Detection](http://openaccess.thecvf.com/content_ECCV_2018/papers/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper.pdf)

>>> ### attention机制

![](/assets/images/detection/23.png)
&emsp;&emsp;mAP比Faster RCNN要好, FPS还比YOLO要高. 总体感觉, 整体思想其实跟YOLO没什么本质区别, 只是增加了更多的数据增广手段, 如多level的feature、随机取patch, 提升了不同尺度下物体的效果. YOLO中对anchor未做长宽比限制, 也就是说每个anchor的形状都是随机无假设的, 而SSD做了不同宽高比的anchor假设, 这里就存在一个先验假设, 假设了物体的不同尺寸, 这个假设对结果有一定的提升. 说白了, 就是trick比较多, 对结果提升有好处, 坏处是很多超参(如宽高比、feature的选择)的调整是手动的, 依据经验的.  
<br
/>
&emsp;&emsp;SSD原文: [SSD](https://arxiv.xilesou.top/pdf/1512.02325.pdf)

***
>+ # ThunderNet (ICCV 2019)

&emsp;&emsp;旷视做了很多模型轻量化的探索, 比如著名的shuffleNet系列, 主要是针对将来部署在端上(手机或摄像头)的实时模型需求. 这篇ThunderNet就是在这种业务背景下催生出来的, 虽然是two-stage, 但是能够在ARM平台上real-time运行.
![](/assets/images/detection/24.png)

>> ### 网络结构

&emsp;&emsp;**Backbone Part:** 320x320固定大小的输入. 传统detection模型的backbone都是从一个classifier网络(例如VGG)迁移而来, 但文中指出`classification`和`detection`任务的性质不同, 具体来说, `classification`需要大的感受域和high level的feature来进行分类, 不关心local的纹理细节, 而`detection`需要low-level的纹理细节(或者说空间信息)来进行定位,  同时又要求大的感受域使得看到物体的范围更大. 因此`detection`所用的网络应该是又浅又大的(层数少且感受域大), 文中的backbone采用了ShuffleNet V2, 但是为了增加low-level的重要性, 将其中3x3的Conv换成了5x5的Conv, 还在low-level feature上增加了更多的channel数. 以上便是被称为SNet的Backbone.  
&emsp;&emsp;**Detection Part:**   
&emsp;&emsp;-- Context Enhancement Module: backbone的输出感受域还是不够大, 在这里, 为了进一步扩大感受域, 采用了金字塔型的多级feature融合方法, 即在backbone的末级用1x1的Conv和上采样得到local context information和global context information.  
![](/assets/images/detection/25.png)
&emsp;&emsp;-- RPN & subnet: RPN与Faster RCNN中的结构差不多, subnet仅仅是一个卷积层和一个全连接层.   
&emsp;&emsp;-- Spatial Attention Module: 这个模块是一个与RPN并行的分支旁路, 它的目的是利用RPN所学得的前景和背景的区别, 增强网络对前景的关注, 降低对背景的关注度. 同时, 它也是一种short cut, 能增强梯度的流动.  
![](/assets/images/detection/26.png)

>> ### 损失函数

&emsp;&emsp;文中并未提及loss函数, 猜测和同样是two-stage的Faster RCNN的loss差不多.

>> ### 模型效果

![](/assets/images/detection/27.png)

>> ### 模型评价

&emsp;&emsp;因为是旷视系的文章, 所以文章倾向于使用或引用孙剑老师门下的文章结构(如GCN、shuffleNet、Lighthead r-cnn等). 同时, 依靠一些对网络模型的理解和经验, 压缩了一些channel数减小计算压力, 增大了一些kernel大小增大了感受域, 设计了一些模块提取local和global的空间信息, 并调整了特征图的注意力分布, 在容量上考虑了backbone和detector的平衡, 各种考量综合起来达到了想要的目的. 虽然可能在模型创新上不是break through, 但是具有很强的实用价值, 可以实时运行在ARM上. 所以要多看文章, 理解神经网络模型的意义, 各个feature对应着什么, 各种任务需要的是什么, 才能合理"搭积木".  

<br
/>
&emsp;&emsp;ThunderNet原文: [ThunderNet](https://arxiv.xilesou.top/pdf/1903.11752.pdf)

